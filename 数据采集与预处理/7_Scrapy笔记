
本实例包括以下几个步骤：
新建工程；
编写代码文件items.py；
编写爬虫文件；
编写代码文件pipelines.py；
编写代码文件settings.py；
运行程序；
把数据保存到数据库中。

1.新建工程
在PyCharm中新建一个名称为“scrapyProject”的工程。
在“scrapyProject”工程底部打开Terminal窗口（如图3-8所示）
，在命令提示符后面输入命令“pip install scrapy”，下载Scrapy框架所需文件。

下载完成后，继续输入命令“scrapy startproject poemScrapy”，
创建Scrapy爬虫框架相关目录和文件。创建完成以后的具体目录结构如图3-9所示，
这些目录和文件都是由Scrapy框架自动创建的，不需要手动创建。

在Scrapy爬虫程序目录结构中，各个目录和文件的作用如下：
1.Spiders目录：该目录下包含爬虫文件，需编码实现爬虫过程；
2.__init__.py：为Python模块初始化目录，可以什么都不写，但是必须要有；
3.items.py：模型文件，存放了需要爬取的字段；
4.middlewares.py：中间件（爬虫中间件、下载中间件），本案例中不用此文件；
5.pipelines.py：管道文件，用于配置数据持久化，例如写入数据库；
6.settings.py：爬虫配置文件；
7.scrapy.cfg：项目基础设置文件，设置爬虫启用功能等。在本案例中不用此文件。

2.编写代码文件items.py
import scrapy

class PoemscrapyItem(scrapy.Item):
    # 名句
    sentence = scrapy.Field()
    # 出处
    source = scrapy.Field()
    # 全文链接
    url = scrapy.Field()
    # 名句详细信息
    content = scrapy.Field()

3.编写爬虫文件
在Terminal窗口输入命令“cd poemScrapy”，进入对应的爬虫工程中，
再输入命令“scrapy genspider poemSpider gushiwen.cn”，这时，
在spiders目录下会出现一个新的Python文件poemSpider.py，
该文件就是我们要编写爬虫程序的位置。下面是poemSpider.py的具体代码：


4.编写代码文件pipelines.py
当我们成功获取需要的信息后，要对信息进行存储。在Scrapy爬虫框架中，
当item被爬虫收集完后，将会被传递到pipelines。现在要将爬取到的数据保存到文本文件中，
可以使用的pipelines.py代码：

5.编写代码文件settings.py
其中，更改USER-AGENT和ROBOTSTXT_OBEY是为了避免访问被拦截或出错；
设置LOG_LEVEL是为了避免在爬取过程中显示过多的日志信息；
设置ITEM_PIPELINES是因为本案例使用到pipeline，
需要先注册pipeline，右侧的数字‘1’为该pipeline的优先级，
范围1-1000，数值越小越优先执行。读者也可以根据实际需求，
适当更改settings.py中的内容。


6.运行程序
有两种执行Scrapy爬虫的方法，
第一种是在Terminal窗口中输入命令“scrapy crawl poemSpider”，
然后回车运行，等待几秒钟后即可完成数据的爬取。

第二种是在poemScrapy目录下
新建Python文件run.py（run.py应与scrapy.cfg文件在同一层目录下），
并输入下面代码：
from scrapy import cmdline
cmdline.execute("scrapy crawl poemSpider".split())
在run.py代码区域点击鼠标右键，在弹出的菜单里选择“Run”运行代码，
就可以执行Scrapy爬虫程序。

7．把数据保存到数据库中
为了把爬取到的数据保存到MySQL数据库中，需要首先安装PyMySQL模块。
在PyCharm开发界面中点击“File->Settings…”，在打开的设置界面中
（如图3-10所示），先点击“Project scrapyProject”，再点击“Python Interpreter”，
会弹出如图3-11所示的设置界面，点击界面底部的“+”。
